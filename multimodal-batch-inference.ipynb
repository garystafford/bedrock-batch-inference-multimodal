{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb6374e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Multimodal Batch Inference on Amazon Bedrock\n",
    "\n",
    "This notebook is based on the original amazon-bedrock-samples notebook: <https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/batch_api/batch-inference-transcript-summarization.ipynb>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59cb329-9bf8-4d6d-85f5-242e087e9bcb",
   "metadata": {},
   "source": [
    "## Install required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f646a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pip -Uq\n",
    "!python -m pip install boto3 botocore -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d30d2d5-602f-4ed1-9b8c-ed12e2f6cf5a",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1321d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b2e50e-1b39-4118-9e43-cf99a485b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# Create Bedrock client for batch inference job\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Constants (CHANGE ME!)\n",
    "ROLE_ARN = \"<your_role_arn>\"\n",
    "BUCKET_NAME = \"<your_bucket_name>\"\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "INPUT_PREFIX = \"batch_inference_data\"\n",
    "OUTPUT_PREFIX = \"job_outputs\"\n",
    "BATCH_FILE_PREFIX = \"batch-image-descriptions\"\n",
    "LOCAL_IMAGE_DIRECTORY = \"images\"\n",
    "LOCAL_BATCH_DATA = \"batch_inference_data\"\n",
    "\n",
    "# Prompts\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert food writer and culinary storyteller specializing in vivid, sensory-rich descriptions of food imagery. \n",
    "With your deep knowledge of global cuisines, cooking techniques, and food photography, you craft compelling narratives that bring dishes to life through precise, evocative language. \n",
    "You balance technical accuracy with artistic flair, incorporating details about texture, color, composition, and presentation while making each description uniquely engaging. \n",
    "Your writing style is warm and accessible while maintaining professional expertise.\"\"\"\n",
    "\n",
    "USER_PROMPT_TITLE = (\n",
    "    \"Create a compelling title for this image of no more than 7-10 words.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_DESCRIPTION = \"Write a brief description for this image.\"\n",
    "\n",
    "USER_PROMPT_KEYWORDS = \"\"\"Generate a list of 15-20 descriptive tags or short phrases that capture key visual elements of this image. \n",
    "Consider all aspects of the image, including visual content, colors, mood, style, and composition. Your output should be a comma-delimited list.\n",
    "\n",
    "Format your response as a single line of comma-separated tags, ordered from most to least prominent. Do not use numbering or bullet points. Do not end the list with a period.\n",
    "\n",
    "Example output:\n",
    "sunlit forest, vibrant green foliage, misty atmosphere, dappled light, towering trees, forest floor, earthy tones, morning dew, \n",
    "tranquil mood, nature photography, depth of field, vertical composition, organic patterns, woodland creatures, biodiversity, \n",
    "environmental theme, soft focus background, wide-angle shot, seasonal change, ethereal quality\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52013093",
   "metadata": {},
   "source": [
    "## Prepare data for the batch inference\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97a6b8f4-2dda-4087-a9f0-4ba743083438",
   "metadata": {},
   "source": [
    "### Formatting input data\n",
    "\n",
    "The input data should be in JSONL format, with each line representing a single transcript for summarization. Each line in your JSONL file should follow this structure:\n",
    "\n",
    "```json\n",
    "{\"recordId\": \"11 character alphanumeric string\", \"modelInput\": {JSON body}}\n",
    "```\n",
    "\n",
    "Here, `recordId` is an 11-character alphanumeric string, working as a unique identifier for each entry. If you omit this field, the batch inference job will automatically add it in the output.\n",
    "\n",
    "The format of the `modelInput` JSON object should match the body field for the model you are using in the `InvokeModel` request. For example, if you're using the Anthropic Claude 3.5 model on Amazon Bedrock, you should use the MessageAPI, and your model input might look like the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"recordId\": \"IMG00000001\",\n",
    "  \"modelInput\": {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"system\": \"You are an expert at writing descriptions of images.\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.3,\n",
    "    \"top_p\": 0.1,\n",
    "    \"top_k\": 100,\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Write a description for this image.\"\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "              \"type\": \"base64\",\n",
    "              \"media_type\": \"image/jpeg\",\n",
    "              \"data\": \"/9j/4AAQSkZJRgABAQEASABIAAD/4gIcSUNDX1BST0Z...\"\n",
    "            }\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1374d-d918-4608-b29e-a571bf8b7a31",
   "metadata": {},
   "source": [
    "### Generating model inputs\n",
    "\n",
    "The `prepare_model_inputs` function reads the input text files from an Amazon S3 bucket, generates unique record IDs, and prepares the model inputs according to the Anthropic Claude 3 model format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8867db-7de1-4a8a-8497-eb9c30e9dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDGenerator:\n",
    "    def __init__(self, prefix=\"IMG\", padding=8, start=1):\n",
    "        \"\"\"\n",
    "        Initialize the ID Generator\n",
    "\n",
    "        Args:\n",
    "            prefix (str): Prefix for the ID (default: \"IMG\")\n",
    "            padding (int): Number of digits to pad with zeros (default: 8)\n",
    "            start (int): Starting number for the counter (default: 1)\n",
    "        \"\"\"\n",
    "        self.prefix = prefix\n",
    "        self.padding = padding\n",
    "        self.counter = start\n",
    "\n",
    "    def get_next_id(self) -> str:\n",
    "        \"\"\"Generate the next ID in sequence\"\"\"\n",
    "        # Format the number with leading zeros\n",
    "        number = str(self.counter).zfill(self.padding)\n",
    "        # Create the ID by combining prefix and padded number\n",
    "        id_string = f\"{self.prefix}{number}\"\n",
    "        # Increment the counter\n",
    "        self.counter += 1\n",
    "        return id_string\n",
    "\n",
    "    def reset_counter(self, value=1) -> None:\n",
    "        \"\"\"Reset the counter to a specific value\"\"\"\n",
    "        self.counter = value\n",
    "\n",
    "    def get_current_counter(self) -> int:\n",
    "        \"\"\"Get the current counter value\"\"\"\n",
    "        return self.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87908a1b-9a09-4e40-8255-2468802d318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path) -> str:\n",
    "    \"\"\"\n",
    "    Encodes an image from the given file path to a base64 string.\n",
    "    Args:\n",
    "        image_path (str): The file path to the image to be encoded.\n",
    "    Returns:\n",
    "        str: The base64 encoded string representation of the image.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "        encoded_string = base64.b64encode(image_data)\n",
    "        return encoded_string.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06f3ea-0563-4deb-8bfc-82adb01ee798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an instance of the class with a prefix of \"IMG\" and padding of 8 digits\n",
    "id_gen = IDGenerator(prefix=\"IMG\", padding=8, start=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400e5e25-a19d-4ae9-93bd-abe28dfb8145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_model_input(image_path) -> str:\n",
    "    \"\"\"\n",
    "    Prepares the input for the Anthropic Claude 3.5 model by creating a request body\n",
    "    that includes a text prompt and an image encoded in base64 format.\n",
    "    Args:\n",
    "        image_path (str): The file path to the image that needs to be described.\n",
    "    Returns:\n",
    "        str: A dictionary containing the record ID and the model input formatted\n",
    "             for the Anthropic Claude 3.5 model.\n",
    "    \"\"\"\n",
    "\n",
    "    body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"system\": SYSTEM_PROMPT,\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.3,\n",
    "        \"top_p\": 0.1,\n",
    "        \"top_k\": 100,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": USER_PROMPT_DESCRIPTION,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/jpeg\",\n",
    "                            \"data\": encode_image(image_path),\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Prepare the model input\n",
    "    model_input = {\"recordId\": id_gen.get_next_id(), \"modelInput\": body}\n",
    "\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c393e5d-bdef-4f1a-ab0d-ac5933256b50",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model_inputs list\n",
    "model_inputs = []\n",
    "\n",
    "# Get sorted list of only the .jpeg images in the directory\n",
    "images = [\n",
    "    image\n",
    "    for image in sorted(os.listdir(LOCAL_IMAGE_DIRECTORY))\n",
    "    if image.endswith(\".jpeg\")\n",
    "]\n",
    "\n",
    "# Iterate over each image in the list up to the batch size\n",
    "batch_size = 100\n",
    "for image in images[:batch_size]:\n",
    "    image_path = os.path.join(LOCAL_IMAGE_DIRECTORY, image)\n",
    "    model_input = prepare_model_input(image_path)\n",
    "\n",
    "    # Append the model input to the list\n",
    "    model_inputs.append(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913ba43",
   "metadata": {},
   "source": [
    "### Writing to JSONL file\n",
    "\n",
    "The `write_jsonl` function takes a list of data (in this case, the list of model inputs) and a file path, and writes the data to a local JSONL file.\n",
    "\n",
    "For each item in the data list, the function converts the item to a JSON string using `json.dumps` and writes it to the file, followed by a newline character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(data, file_path) -> None:\n",
    "    \"\"\"\n",
    "    Writes a list of dictionaries to a file in JSON Lines format.\n",
    "    Args:\n",
    "        data (list): A list of dictionaries to be written to the file.\n",
    "        file_path (str): The path to the file where the data will be written.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for item in data:\n",
    "            json_str = json.dumps(item)\n",
    "            file.write(json_str + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write model inputs to the .jsonl batch inference data file\n",
    "batch_file_name = (\n",
    "    f\"{BATCH_FILE_PREFIX}-{batch_size}-{int(datetime.now().timestamp())}.jsonl\"\n",
    ")\n",
    "logger.info(batch_file_name)\n",
    "\n",
    "write_jsonl(model_inputs, f\"./{LOCAL_BATCH_DATA}/{batch_file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d5bf1-4ad3-4491-a8b2-4703bc830c69",
   "metadata": {},
   "source": [
    "### Uploading to Amazon S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cbedd-f505-495d-b40a-0ecf572fd5a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(path, bucket_name, bucket_subfolder=None) -> bool:\n",
    "    \"\"\"\n",
    "    Uploads a file or directory to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        path (str): The local path to the file or directory to upload.\n",
    "        bucket_name (str): The name of the S3 bucket to upload to.\n",
    "        bucket_subfolder (str, optional): The subfolder within the S3 bucket to upload to. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the upload was successful, False if there was an error uploading a file,\n",
    "              or None if the path is not a file or directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the path is a file\n",
    "    if os.path.isfile(path):\n",
    "        # If the path is a file, upload it directly\n",
    "        object_name = (\n",
    "            os.path.basename(path)\n",
    "            if bucket_subfolder is None\n",
    "            else f\"{bucket_subfolder}/{os.path.basename(path)}\"\n",
    "        )\n",
    "        try:\n",
    "            s3.upload_file(path, bucket_name, object_name)\n",
    "            logger.info(f\"Successfully uploaded {path} to {bucket_name}/{object_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error uploading {path} to S3: {e}\")\n",
    "            return False\n",
    "    elif os.path.isdir(path):\n",
    "        # If the path is a directory, recursively upload all files within it\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(file_path, path)\n",
    "                object_name = (\n",
    "                    relative_path\n",
    "                    if bucket_subfolder is None\n",
    "                    else f\"{bucket_subfolder}/{relative_path}\"\n",
    "                )\n",
    "                try:\n",
    "                    s3.upload_file(file_path, bucket_name, object_name)\n",
    "                    logger.info(\n",
    "                        f\"Successfully uploaded {file_path} to {bucket_name}/{object_name}\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error uploading {file_path} to S3: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        logger.warning(f\"{path} is not a file or directory.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b688b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data from local to S3 bucket for batch inference\n",
    "upload_to_s3(\n",
    "    path=f\"./{LOCAL_BATCH_DATA}/{batch_file_name}\",\n",
    "    bucket_name=BUCKET_NAME,\n",
    "    bucket_subfolder=INPUT_PREFIX,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586aacfb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Creating the Batch Inference Job\n",
    "\n",
    "Once the data is prepared and uploaded to an Amazon S3, you can create the batch inference job.\n",
    "\n",
    "### Configuring input and output data\n",
    "\n",
    "Before submitting the batch inference job, you need to configure the input and output data locations in Amazon S3. This is done using the `inputDataConfig` and `outputDataConfig` parameters.\n",
    "\n",
    "The `inputDataConfig` specifies the Amazon S3 URI where the prepared input data (JSONL file) is stored and, the `outputDataConfig` specifies the Amazon S3 URI where the processed output data will be stored by the batch inference job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a50314",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDataConfig = {\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{BUCKET_NAME}/{INPUT_PREFIX}/{batch_file_name}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "outputDataConfig = {\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": f\"s3://{BUCKET_NAME}/{INPUT_PREFIX}/{OUTPUT_PREFIX}/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "logger.info(inputDataConfig)\n",
    "logger.info(outputDataConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06129a4a",
   "metadata": {},
   "source": [
    "### Submitting the Batch Inference Job\n",
    "\n",
    "To submit the batch inference job, you use the `create_model_invocation_job` API from the Amazon Bedrock client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f\"batch-job-{batch_size}-{str(int(datetime.now().timestamp()))}\"\n",
    "logger.info(job_name)\n",
    "\n",
    "response = bedrock.create_model_invocation_job(\n",
    "    roleArn=ROLE_ARN,\n",
    "    modelId=MODEL_ID,\n",
    "    jobName=job_name,\n",
    "    inputDataConfig=inputDataConfig,\n",
    "    outputDataConfig=outputDataConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d3b15",
   "metadata": {},
   "source": [
    "### Monitoring job status\n",
    "\n",
    "After submitting the batch inference job, you can monitor its status using the `get_model_invocation_job` API from the Amazon Bedrock client. This API requires the `jobIdentifier` parameter, which is the ARN of the submitted job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ed4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_arn = response.get(\"jobArn\")\n",
    "job_id = job_arn.split(\"/\")[1]\n",
    "\n",
    "logger.info(f\"Job ARN: {job_arn}\")\n",
    "\n",
    "status = \"\"\n",
    "while status not in [\"Completed\", \"Failed\"]:\n",
    "    job_response = bedrock.get_model_invocation_job(jobIdentifier=job_arn)\n",
    "    status = job_response[\"status\"]\n",
    "    if status == \"Failed\":\n",
    "        logger.info(job_response)\n",
    "    elif status == \"Completed\":\n",
    "        logger.info(f\"Status: {status}\")\n",
    "        break\n",
    "    else:\n",
    "        logger.info(f\"Status: {status}\")\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62690e19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Retrieving and analyzing output\n",
    "\n",
    "When your batch inference job is complete, Amazon Bedrock creates a dedicated folder in the specified S3 bucket, using the job ID as the folder name. This folder contains a summary of the batch inference job, along with the processed inference data in JSONL format.\n",
    "\n",
    "### Accessing and understanding output format\n",
    "\n",
    "The output files contain the processed text, observability data, and the parameters used for inference. The format of the output data will depend on the model you used for batch inference. The notebook provides an example of how to access and process this information from the output JSONL file for Anthropic Claude 3.5 models.\n",
    "\n",
    "Additionally, in the output location specified for your batch inference job, you'll find a `manifest.json.out` file that provides a summary of the processed records. This file includes information such as the total number of records processed, the number of successfully processed records, the number of records with errors, and the total input and output token counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19072d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the S3 bucket name and prefix for the text files.\n",
    "# Last part in the path is the batch job's job id\n",
    "prefix = f\"{INPUT_PREFIX}/{OUTPUT_PREFIX}/{job_id}/\"\n",
    "\n",
    "# Initialize the list\n",
    "output_data = []\n",
    "\n",
    "# Read the JSON file from S3\n",
    "try:\n",
    "    object_key = f\"{prefix}{batch_file_name}.out\"\n",
    "    response = s3.get_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=object_key,\n",
    "    )\n",
    "    json_data = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    output_entry = None\n",
    "\n",
    "    # Process the JSON data and output the first entry\n",
    "    lines = json_data.splitlines()\n",
    "    for line in lines[0:1]:\n",
    "        data = json.loads(line)\n",
    "        output_entry = {\n",
    "            \"request_id\": data[\"modelOutput\"][\"id\"],\n",
    "            \"output_text\": data[\"modelOutput\"][\"content\"][0][\"text\"],\n",
    "            \"observability\": {\n",
    "                \"input_tokens\": data[\"modelOutput\"][\"usage\"][\"input_tokens\"],\n",
    "                \"output_tokens\": data[\"modelOutput\"][\"usage\"][\"output_tokens\"],\n",
    "                \"model\": data[\"modelOutput\"][\"model\"],\n",
    "                \"stop_reason\": data[\"modelOutput\"][\"stop_reason\"],\n",
    "                \"recordId\": data[\"recordId\"],\n",
    "                \"max_tokens\": data[\"modelInput\"][\"max_tokens\"],\n",
    "                \"temperature\": data[\"modelInput\"][\"temperature\"],\n",
    "                \"top_p\": data[\"modelInput\"][\"top_p\"],\n",
    "                \"top_k\": data[\"modelInput\"][\"top_k\"],\n",
    "            },\n",
    "        }\n",
    "        output_data.append(output_entry)\n",
    "    logger.info(f\"Successfully read {len(output_data)} JSON objects from S3.\")\n",
    "    logger.info(json.dumps(output_entry, indent=4))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading JSON file from S3: {e}\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
